{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../')))\n",
    "from sklearn.decomposition import PCA\n",
    "from optimizers.AdaFisher import AdaFisher\n",
    "from optimizers.kfac import KFACOptimizer\n",
    "from asdl.precondition import PreconditioningConfig, ShampooGradientMaker\n",
    "from optimizers.AdaHessian import Adahessian\n",
    "from optimizers.Adam import Adam\n",
    "from optimizers.sgd import SGD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and apply Data redution and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "data = iris['data']\n",
    "target = iris['target']\n",
    "pca = PCA(n_components=2)\n",
    "data_red = pca.fit_transform(data)\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_red)\n",
    "# Convert to torch tensors\n",
    "data_tensor = torch.tensor(data_scaled, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(target, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the IRIS dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = IrisDataset(data_tensor, labels_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Toy MLP classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, target_size: int):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 1)\n",
    "        self.fc2 = nn.Linear(1, target_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.relu = nn.ReLU()\n",
    "        with torch.no_grad():\n",
    "            self.fc1.weight.fill_(0.5)  # Set all weights to 0.5\n",
    "            self.fc1.bias.zero_() \n",
    "            self.fc2.weight.fill_(0.5)  # Set all weights to 0.5\n",
    "            self.fc2.bias.zero_() \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function where the loss and the parameters of the first layer are catched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_AdaHesian =  MLP(target_size=3)\n",
    "model_Adam =  MLP(target_size=3)\n",
    "model_AdaFisher =  MLP(target_size=3)\n",
    "model_Shampoo =  MLP(target_size=3)\n",
    "model_kfac =  MLP(target_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, loss_fn, epochs, gm = None):\n",
    "    model.fc1.weight.data = torch.Tensor([[0.2], [0.2]]).reshape(1,2)\n",
    "    weight_history = []\n",
    "    loss_history = []\n",
    "    for _ in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            if optimizer.__class__.__name__ == \"SGD\":\n",
    "                dummy_y = gm.setup_model_call(model, inputs)\n",
    "                gm.setup_loss_call(loss_fn, dummy_y, targets)\n",
    "                outputs, loss = gm.forward_and_backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                if optimizer.__class__.__name__ == \"Adahessian\":\n",
    "                    loss.backward(create_graph=True)\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        weight_history.append(model.fc1.weight.data.cpu().numpy().flatten())\n",
    "        loss_history.append(total_loss / len(train_loader))\n",
    "    return weight_history, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train for SGD, AdaFisher and Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MLP(target_size=3)\n",
    "config = PreconditioningConfig(data_size=10, \n",
    "                                           damping=1e-12,\n",
    "                                           preconditioner_upd_interval=1,\n",
    "                                           curvature_upd_interval=1,\n",
    "                                           ema_decay=-1,\n",
    "                                           ignore_modules=[nn.BatchNorm1d,nn.BatchNorm2d,nn.BatchNorm3d,nn.LayerNorm])\n",
    "\n",
    "optimizers = {\n",
    "    'KFAC': KFACOptimizer(model_kfac, lr=0.001),\n",
    "    'Adam': Adam(model_Adam.parameters(), lr=0.001),\n",
    "    'AdaFisher': AdaFisher(model_AdaFisher, lr=0.001),\n",
    "    'AdaHessian': Adahessian(model_AdaHesian.parameters(), lr=0.01),\n",
    "    'Shampoo': SGD(model_Shampoo.parameters(), lr=0.001, momentum=0.9)\n",
    "}\n",
    "config = PreconditioningConfig(data_size=10, \n",
    "                                           damping=1e-12,\n",
    "                                           preconditioner_upd_interval=100,\n",
    "                                           curvature_upd_interval=100,\n",
    "                                           ema_decay=-1,\n",
    "                                           ignore_modules=[nn.BatchNorm1d,nn.BatchNorm2d,nn.BatchNorm3d,nn.LayerNorm])\n",
    "loss = nn.CrossEntropyLoss()\n",
    "device = 'cpu'\n",
    "EPOCHS = 20\n",
    "optimizer_results = {}\n",
    "weight = np.zeros((100, 2))\n",
    "for name, opt in tqdm(optimizers.items()):\n",
    "    # Reset model for each optimizer\n",
    "    if name == \"Shampoo\":\n",
    "        model_Shampoo.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
    "        gm = ShampooGradientMaker(model_Shampoo, config)\n",
    "        optimizer_results[name] = train_model(model_Shampoo, train_loader, opt, loss, EPOCHS, gm)\n",
    "    elif name == \"Adam\":\n",
    "        model_Adam.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
    "        optimizer_results[name] = train_model(model_Adam, train_loader, opt, loss, EPOCHS)\n",
    "    elif name == \"AdaFisher\":\n",
    "        model_AdaFisher.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
    "        optimizer_results[name] = train_model(model_AdaFisher, train_loader, opt, loss, EPOCHS)\n",
    "    elif name == \"AdaHessian\":\n",
    "        model_AdaHesian.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
    "        optimizer_results[name] = train_model(model_AdaHesian, train_loader, opt, loss, EPOCHS)\n",
    "    elif name == \"KFAC\":\n",
    "        model_kfac.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
    "        optimizer_results[name] = train_model(model_kfac, train_loader, opt, loss, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_landscape_visualization(zoom_window: bool = False):\n",
    "    # Assuming optimizer_results is predefined as in your script\n",
    "    weights_flat = np.vstack([weights for _, (weights, _) in optimizer_results.items()])\n",
    "    loss_flat = np.concatenate([losses for _, (_, losses) in optimizer_results.items()])\n",
    "\n",
    "    grid_x, grid_y = np.mgrid[min(weights_flat[:,0]):max(weights_flat[:,0]):200j, min(weights_flat[:,1]):max(weights_flat[:,1]):200j]\n",
    "    grid_z = griddata(weights_flat, loss_flat, (grid_x, grid_y), method='cubic')\n",
    "\n",
    "    plt.figure(figsize=(12, 10))  # Larger figure size\n",
    "\n",
    "    contour = plt.contourf(grid_x, grid_y, grid_z, levels=1000, cmap='jet', alpha=0.3)  # Smoother contour, better colormap\n",
    "    cbar = plt.colorbar(contour, pad=0.01)\n",
    "    cbar.formatter = FormatStrFormatter('%.2f')  # Format with two decimals\n",
    "    cbar.update_ticks()  # Update ticks to use the new formatter\n",
    "    cbar.ax.set_ylabel('Loss Value', fontsize=24)\n",
    "    cbar.ax.tick_params(labelsize='large')\n",
    "\n",
    "    colors = {'AdaFisher': 'black', 'KFAC': 'purple', 'Adam': 'cyan', 'AdaHessian': 'green', \"Shampoo\": \"blue\"}\n",
    "    for name, (weights, _) in optimizer_results.items():\n",
    "        weights = np.vstack(weights)\n",
    "        diff_weights = np.diff(weights, axis=0)\n",
    "        plt.quiver(weights[:-1, 0], weights[:-1, 1], diff_weights[:, 0], diff_weights[:, 1], scale_units='xy', angles='xy', scale=1,\n",
    "                color=colors[name], label=name, width=0.004, alpha=1)\n",
    "\n",
    "    # Zoomed-in window\n",
    "    plt.title('Weight Trajectories on Loss Landscape', fontsize=24, fontweight='bold')\n",
    "    plt.xlabel('$W_1$', fontsize=30, fontweight='bold')\n",
    "    plt.ylabel('$W_2$', fontsize=30, fontweight='bold')\n",
    "    plt.legend(fontsize='xx-large', loc='upper right')\n",
    "    plt.tick_params(labelsize='large')\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    if zoom_window:\n",
    "        axins = plt.axes([0.1, 0.15, 0.25, 0.25], facecolor='w')\n",
    "        zoom_area = {'KFAC': 'purple', 'Adam': 'cyan'}\n",
    "        for name in ['KFAC', 'Adam']:\n",
    "            weights = np.vstack(optimizer_results[name][0])\n",
    "            plt.sca(axins)\n",
    "            axins.contourf(grid_x, grid_y, grid_z, levels=1000, cmap='jet', alpha=0.3)\n",
    "            diff_weights = np.diff(weights, axis=0)\n",
    "            axins.quiver(weights[:-1, 0], weights[:-1, 1], diff_weights[:, 0], diff_weights[:, 1], scale_units='xy', angles='xy', scale=1,\n",
    "                        color=zoom_area[name], label=name, width=0.01, alpha=1)\n",
    "            axins.set_xlim(min(weights[:, 0]), max(weights[:, 0])+ 0.3)\n",
    "            axins.set_ylim(min(weights[:, 1]), max(weights[:, 1]))\n",
    "            axins.set_xticklabels([])\n",
    "            axins.set_yticklabels([])\n",
    "            axins.set_xticks([])  # Remove x-axis ticks\n",
    "            axins.set_yticks([])  # Remove y-axis ticks\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_landscape_visualization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
